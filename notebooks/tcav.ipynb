{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593b6e6d",
   "metadata": {},
   "source": [
    "# Testing with Concept Activation Vectors (TCAV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea681bac",
   "metadata": {},
   "source": [
    "Introduced by [Kim et al. (2018)](https://arxiv.org/pdf/1711.11279.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c0f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "model = resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bc059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concept = \"cup\"\n",
    "concept = \"chair\"\n",
    "#k_class = \"bottle\"\n",
    "k_class = \"dining table\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994220f",
   "metadata": {},
   "source": [
    "## Get images for classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a2e6e",
   "metadata": {},
   "source": [
    "We will use the [pycocotools](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb) API to retrieve some examples of chairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "coco = COCO(\"./datasets/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91b00c",
   "metadata": {},
   "source": [
    "Let's get the IDs of the images in the coco dataset that have the concept of _chair_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_cat_id = coco.getCatIds(catNms=[concept])\n",
    "concept_imgs_ids = coco.getImgIds(catIds=concept_cat_id)\n",
    "len(concept_imgs_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ded06",
   "metadata": {},
   "source": [
    "Let's remove the images where chairs are accompanied by dining table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_cats_ids = coco.getCatIds(catNms=[concept, k_class])\n",
    "overlap_imgs_ids = coco.getImgIds(catIds=overlap_cats_ids)\n",
    "len(overlap_imgs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2864f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_imgs_ids = [img_id for img_id in concept_imgs_ids if img_id not in overlap_imgs_ids]\n",
    "len(concept_imgs_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654bfbd",
   "metadata": {},
   "source": [
    "Let's grab random images for the other class of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f71530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_imgs_ids = list(coco.imgs.keys())\n",
    "not_concept_imgs_ids = [img_id for img_id in all_imgs_ids if img_id not in concept_imgs_ids]\n",
    "random.Random(0).shuffle(not_concept_imgs_ids)\n",
    "not_concept_imgs_ids = not_concept_imgs_ids[:len(concept_imgs_ids)]\n",
    "\n",
    "assert not any(img_id in not_concept_imgs_ids for img_id in concept_imgs_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f0d22",
   "metadata": {},
   "source": [
    "## Extract activations from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "preprocessing = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60068b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_id = concept_imgs_ids[18]\n",
    "img_url =  coco.loadImgs(img_id)[0][\"coco_url\"]\n",
    "img = io.imread(img_url)\n",
    "img_pil = Image.fromarray(img)\n",
    "prepro_img = preprocessing(img_pil)\n",
    "#type(img)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5354183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import skimage.io as io\n",
    "\n",
    "class_ids = np.concatenate(\n",
    "    (np.zeros(len(concept_imgs_ids)), np.ones(len(not_concept_imgs_ids))),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "all_imgs_ids = np.concatenate((concept_imgs_ids, not_concept_imgs_ids))\n",
    "imgs = []\n",
    "for img_id in all_imgs_ids:\n",
    "    img_url =  coco.loadImgs(int(img_id))[0][\"coco_url\"]\n",
    "    img = io.imread(img_url)\n",
    "    img_pil = Image.fromarray(img).convert(\"RGB\")\n",
    "    img_prepro = preprocessing(img_pil)\n",
    "    img_unsq = img_prepro.unsqueeze(0)\n",
    "    imgs.append(img_unsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_id = 18\n",
    "plt.imshow(imgs[img_id].permute(1, 2, 0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "imgs_tensor = torch.cat(imgs)\n",
    "imgs_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "repr_input = OrderedDict()\n",
    "repr_output = OrderedDict()\n",
    "\n",
    "def get_representation(name):\n",
    "    def hook(model, input, output):\n",
    "        repr_output[name] = output.detach()\n",
    "    return hook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
