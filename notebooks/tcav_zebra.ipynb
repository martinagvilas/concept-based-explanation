{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c32383",
   "metadata": {},
   "source": [
    "# Testing with Concept Activation Vectors (TCAV) - A step by step tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4238c",
   "metadata": {},
   "source": [
    "Testing with Concept Activation Vectors (TCAV) is a concept-based interpretability method introduced by [Kim et al. (2018)](https://arxiv.org/pdf/1711.11279.pdf). It quantitatevely measures how much a pre-defined, human-understandable concept might be influencing the predictions made by a trained deep neural network (DNN).\n",
    "\n",
    "The authors of the paper provide one illustrative example of such question: How does the concept of _stripes_ guides the DNN to predict that an image belongs to the _zebra_ class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757e264",
   "metadata": {},
   "source": [
    "## 1) Define the concept and class of interest\n",
    "\n",
    "We start by manually selecting a set of example images containing the concept of interest. There images can reflect the concept in different ways. In the case of stripes, for example, they can be pictures of the texture, or images containing striped objects. These images also don't need to be part of the training set of the DNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b70d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = \"striped\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a34e19",
   "metadata": {},
   "source": [
    "Let's create a function for loading the images of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dfdf75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "preprocessing = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "def get_images_input(images_path, transform):\n",
    "    imgs_files = list(Path(images_path).iterdir())\n",
    "    \n",
    "    prepro_imgs = []\n",
    "    for file in imgs_files:\n",
    "        img = Image.open(file).convert(\"RGB\")\n",
    "        img_prepro = transform(img)\n",
    "        img_unsq = img_prepro.unsqueeze(0)\n",
    "        prepro_imgs.append(img_unsq)\n",
    "    \n",
    "    imgs_tensor = torch.cat(prepro_imgs)\n",
    "    \n",
    "    return imgs_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284dc653",
   "metadata": {},
   "source": [
    "Let's load the concept images and random images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646a3dcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/martina.gonzales/data/tcav/image/concepts/striped'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zk/25nqh7xn2lz329c845__w5nm0000gp/T/ipykernel_28140/3066484113.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconcept_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_images_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/Users/martina.gonzales/data/tcav/image/concepts/{concept}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrandom_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_images_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/Users/martina.gonzales/data/tcav/image/concepts/random_0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of concept images input: {concept_images.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of random images input: {concept_images.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zk/25nqh7xn2lz329c845__w5nm0000gp/T/ipykernel_28140/1322163897.py\u001b[0m in \u001b[0;36mget_images_input\u001b[0;34m(images_path, transform)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_images_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mimgs_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprepro_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/cbe/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/martina.gonzales/data/tcav/image/concepts/striped'"
     ]
    }
   ],
   "source": [
    "concept_images = get_images_input(f\"/Users/martina.gonzales/data/tcav/image/concepts/{concept}\", preprocessing)\n",
    "random_images = get_images_input(f\"/Users/martina.gonzales/data/tcav/image/concepts/random_0\", preprocessing)\n",
    "\n",
    "print(f\"Shape of concept images input: {concept_images.shape}\")\n",
    "print(f\"Shape of random images input: {concept_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf87a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "zebra_images = get_images_input(f\"/Users/martina.gonzales/data/tcav/image/imagenet/zebra\", preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b6e3f",
   "metadata": {},
   "source": [
    "## 2) Get the DNN's internal representations of the concept\n",
    "\n",
    "The second step is to pass the images of the concept as well as the random images to the pre-trained DNN and extract their internal representations (a.k.a. activations). Usually, we only explore the activations of one or a couple of layers in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc39e7b",
   "metadata": {},
   "source": [
    "For this example we will explore how the concepts activate `layer3` of a `resnet50` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09069894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval();\n",
    "\n",
    "layers = [\"layer3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f450833",
   "metadata": {},
   "source": [
    "There are multiple ways we can extract the internal representations of a network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc96717",
   "metadata": {},
   "source": [
    "Let's create the hooks for the activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representation(mod, inp, output):\n",
    "    output = output.detach()\n",
    "    features.append(output)\n",
    "\n",
    "for layer_name, layer in model.named_modules():\n",
    "    if layer_name in layers:\n",
    "        handle = layer.register_forward_hook(get_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8921417",
   "metadata": {},
   "source": [
    "Let's obtain the activations and create the feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = []\n",
    "\n",
    "for images_input in [concept_images, random_images]:\n",
    "    features = []\n",
    "    out = model(images_input)\n",
    "    features = torch.cat(features)\n",
    "    features = features.reshape((features.shape[0], -1))\n",
    "    feature_matrix.append(features)\n",
    "\n",
    "feature_matrix = torch.cat(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6e1a9",
   "metadata": {},
   "source": [
    "Let's very the shape of the feature matrix is the expected one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6ccfd",
   "metadata": {},
   "source": [
    "Let's create a vector of class ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_ids = np.concatenate(\n",
    "    (np.zeros(len(concept_images)), np.ones(len(random_images))),\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a9446",
   "metadata": {},
   "source": [
    "## 3) compute Concept Activation Vectors (CAVs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831870f",
   "metadata": {},
   "source": [
    "Create classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c84840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, class_ids, test_size=0.33)\n",
    "\n",
    "clf.fit(X_train.detach().numpy(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3d6a6",
   "metadata": {},
   "source": [
    "Let's inspect how accurate was the classifier in distinguishing these concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede4fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e7291",
   "metadata": {},
   "source": [
    "Finally, let's create our CAV vectors, These are the weights of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549281c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cavs = torch.tensor(np.array([-1 * clf.coef_[0], clf.coef_[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042abb2",
   "metadata": {},
   "source": [
    "Let's inspect the shape of the CAVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cavs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f15d9",
   "metadata": {},
   "source": [
    "## 4) compute directional derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ad244",
   "metadata": {},
   "source": [
    "$$S_{C,\\,k,\\,l}\\left(x\\right)\\,=\\,\\lim_{\\epsilon\\to 0}\\frac{h_{l,k}\\left(f_{l}\\left(x\\right)+\\epsilon v_{C}^{l}\\right)-h_{l,k}\\left(f_{l}\\left(x\\right)\\right)}{\\epsilon}$$\n",
    "\n",
    "> where:\n",
    ">\n",
    "> --> $h_{l,k}(x)$ is the logit for a data point $x$ for class $k$ (and l?)\n",
    ">\n",
    "> --> $f_l(x)$ is the activations for input x at layer $l$\n",
    ">\n",
    "> --> $v^l_C$  is a unit CAV vector for a concept C in layer $l$\n",
    "\n",
    "$$S_{C,\\,k,\\,l} = \\triangledown h_{l,k}(f_l(x))\\cdot v^l_C$$\n",
    "\n",
    "> the dot product of the gradient of the logit $h_{l,k}(x)$ at a point $f_l(x)$  with another tangent vector $v^l_C$  equals the directional derivative of $h_{l,k}(x)$ at $f_l(x)$ of the function along $v^l_C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce06434",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "model.eval();\n",
    "# TODO: try handle.remove()\n",
    "\n",
    "#with torch.autograd.set_grad_enabled(True):\n",
    "def get_representation(mod, inp, output):\n",
    "    activations.append(output)\n",
    "\n",
    "model.layer3.register_forward_hook(get_representation)\n",
    "model.fc.register_forward_hook(get_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad52b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "\n",
    "out = model(zebra_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f72717",
   "metadata": {},
   "source": [
    "The variable `activations` should now be a list containing the activations for the layer of our choise, and the obtained logits for all classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Layer activations: {activations[0].shape}\")\n",
    "print(f\"Logits: {activations[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7563541",
   "metadata": {},
   "source": [
    "But we only want to explore the effects on the logit corresponding to our class of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zebra_id = 340\n",
    "\n",
    "logits_class = activations[1][(slice(None), zebra_id)]\n",
    "print(logits_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28acc221",
   "metadata": {},
   "source": [
    "Let's prepare the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8971bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_class = torch.unbind(logits_class)\n",
    "layer_activations = (activations[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202fc2ed",
   "metadata": {},
   "source": [
    "Let's now compute the gradients using the `autograd` functionality of pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = torch.autograd.grad(logits_class, layer_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753540e",
   "metadata": {},
   "source": [
    "The result will be a torch tensor, of the same shape as the layers activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(grads[0]))\n",
    "print(grads[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049e284",
   "metadata": {},
   "source": [
    "We can now obtain our directional derivatives. This is computed by taking the dot product between the gradients and the concept vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tensor into right format for dot product\n",
    "grads_flat = (\n",
    "    torch.squeeze(grads[0].reshape(grads[0].shape[0], -1))\n",
    "    .type(torch.float64)\n",
    ")\n",
    "grads_flat.shape\n",
    "\n",
    "dir_derivative = torch.matmul(grads_flat, cavs[0])\n",
    "dir_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0aa8c9",
   "metadata": {},
   "source": [
    "## test CAVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de075dd",
   "metadata": {},
   "source": [
    "$$TCAV_{Q_{C,k,l}}=\\,\\frac{\\left|\\left\\{x\\epsilon X_{k}:S_{C,k,l}\\left(x\\right)>0\\right\\}\\right|}{\\left|X_{k}\\right|}$$\n",
    "\n",
    "- scores the fraction of k-class inputs whose activation vector on layer l was positively influenced by the concept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcav_score = torch.sum(dir_derivative > 0) / dir_derivative.shape[0]\n",
    "print(f\"TCAV score: {tcav_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0e821",
   "metadata": {},
   "source": [
    "## Statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad3f7a",
   "metadata": {},
   "source": [
    "At least 500 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f050e0e",
   "metadata": {},
   "source": [
    "## Explore results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "cos_sim = []\n",
    "for img_idx in range(len(zebra_images)):\n",
    "    img_activations = layer_activations[0][img_idx].reshape(-1).unsqueeze(dim=0)\n",
    "    cos = cosine_similarity(img_activations, cavs[0].unsqueeze(dim=0))\n",
    "    cos_sim.append(cos)\n",
    "\n",
    "cos_sim = torch.tensor(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vals = torch.argsort(cos_sim)\n",
    "min_vals = sorted_vals[:3]\n",
    "max_vals = sorted_vals[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for val in min_vals:\n",
    "    plt.figure()\n",
    "    plt.imshow(zebra_images[val].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf207b4f",
   "metadata": {},
   "source": [
    "## Disadvantages\n",
    "\n",
    "- Assumes linearity\n",
    "- Manual annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e99f7",
   "metadata": {},
   "source": [
    "-------\n",
    "## Resources\n",
    "\n",
    "- [TCAV with Captum](https://captum.ai/tutorials/TCAV_Image) tutorial.\n",
    "- [Towards better understanding of gradient-based attribution methods for deep neural networks](https://arxiv.org/pdf/1711.06104.pdf) by Ancona et al. (2018), _ICLR_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e90dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
