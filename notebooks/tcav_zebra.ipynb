{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c32383",
   "metadata": {},
   "source": [
    "# Testing with Concept Activation Vectors (TCAV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4238c",
   "metadata": {},
   "source": [
    "Introduced by [Kim et al. (2018)](https://arxiv.org/pdf/1711.11279.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757e264",
   "metadata": {},
   "source": [
    "## 1) prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104de84",
   "metadata": {},
   "source": [
    "We need to:\n",
    "\n",
    "1. Create concepts of interest defined by input images\n",
    "2. Create random concepts\n",
    "3. Define class of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b70d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = \"striped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dfdf75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocessing = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d4b4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_images_input(images_path, transform):\n",
    "    imgs_files = list(Path(images_path).iterdir())\n",
    "    \n",
    "    prepro_imgs = []\n",
    "    for file in imgs_files:\n",
    "        img = Image.open(file).convert(\"RGB\")\n",
    "        img_prepro = transform(img)\n",
    "        img_unsq = img_prepro.unsqueeze(0)\n",
    "        prepro_imgs.append(img_unsq)\n",
    "    \n",
    "    imgs_tensor = torch.cat(prepro_imgs)\n",
    "    \n",
    "    return imgs_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "646a3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_images = get_images_input(f\"/Users/martina.gonzales/data/tcav/image/concepts/{concept}\", preprocessing)\n",
    "random_images = get_images_input(f\"/Users/martina.gonzales/data/tcav/image/concepts/random_0\", preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8edc99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 3, 224, 224])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b6e3f",
   "metadata": {},
   "source": [
    "## Compute activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e89f2",
   "metadata": {},
   "source": [
    "Let's load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09069894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc96717",
   "metadata": {},
   "source": [
    "Let's create the hooks for the activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6a3d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representation(mod, inp, output):\n",
    "    output = output.detach()\n",
    "    features.append(output)\n",
    "\n",
    "handle = model.layer4.register_forward_hook(get_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8921417",
   "metadata": {},
   "source": [
    "Let's obtain the activations and create the feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90ebd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = []\n",
    "\n",
    "for images_input in [concept_images, random_images]:\n",
    "    features = []\n",
    "    out = model(images_input)\n",
    "    features = torch.cat(features)\n",
    "    features = features.reshape((features.shape[0], -1))\n",
    "    feature_matrix.append(features)\n",
    "\n",
    "feature_matrix = torch.cat(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "79b95501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.1773,  ..., 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix[239]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6e1a9",
   "metadata": {},
   "source": [
    "The shape of the feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ea6b471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 100352])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f0f0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_ids = np.concatenate(\n",
    "    (np.zeros(len(concept_images)), np.ones(len(random_images))),\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a69e363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 2048, 7, 7])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy as np\n",
    "test_2 = torch.cat(feature_matrix)\n",
    "test_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a9446",
   "metadata": {},
   "source": [
    "## Compute CAV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831870f",
   "metadata": {},
   "source": [
    "Create classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "739e8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "clf = SGDClassifier(alpha=0.01, max_iter=1000, tol=1e-3, random_state=0)\n",
    "clf = LogisticRegression(C=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "59c84840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, class_ids, test_size=0.33)\n",
    "\n",
    "clf.fit(X_train.detach().numpy(), y_train)\n",
    "\n",
    "preds = torch.tensor(clf.predict(X_test.detach().numpy()))\n",
    "#return {'accs': (preds == y_test).float().mean()}\n",
    "score = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cb0c4f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/2y3g1m_s1kz4x__hr0p6ydx1xnlcbr/T/ipykernel_3281/1463953307.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.argmax(torch.tensor(preds), dim=1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d_/2y3g1m_s1kz4x__hr0p6ydx1xnlcbr/T/ipykernel_3281/1463953307.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "torch.argmax(torch.tensor(preds), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ba9392a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ede4fe78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds.detach().numpy() == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "077b85cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "549281c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cavs = torch.tensor(np.array([-1 * clf.coef_[0], clf.coef_[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3c1942d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.2558e-05, -2.2978e-05, -3.3049e-05,  ..., -1.9948e-04,\n",
       "         -5.2722e-05, -2.0139e-04],\n",
       "        [ 9.2558e-05,  2.2978e-05,  3.3049e-05,  ...,  1.9948e-04,\n",
       "          5.2722e-05,  2.0139e-04]], dtype=torch.float64)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b74a71f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "36a8ad9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100352])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cavs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f15d9",
   "metadata": {},
   "source": [
    "## Compute directional derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b936fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cbe] *",
   "language": "python",
   "name": "conda-env-cbe-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
